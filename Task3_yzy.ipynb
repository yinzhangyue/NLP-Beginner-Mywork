{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 导入数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_snli(data_dir, is_train):\n",
    "    '''\n",
    "    @params:\n",
    "        data_dir: 数据集所在位置\n",
    "        is_train：为True时加载训练集，否则加载测试集\n",
    "    @return: premises：前提\n",
    "             hypotheses：假设\n",
    "             labels：标签\n",
    "    '''\n",
    "    def extract_text(s):\n",
    "        # 移除不必要的信息\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # 用空格代替两个或多个连续空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    '''\n",
    "    蕴涵：假设可以从前提中推断出来。\n",
    "    矛盾：假设的否定可以从前提推断出来。\n",
    "    中立：所有其情况。             \n",
    "    '''\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./snli/\"\n",
    "# 加载训练集\n",
    "train_data = read_snli(data_dir, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is training his horse for a competition .\n",
      "label: 2\n",
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is at a diner , ordering an omelette .\n",
      "label: 1\n",
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is outdoors , on a horse .\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('premise:', x0)\n",
    "    print('hypothesis:', x1)\n",
    "    print('label:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试集\n",
    "test_data = read_snli(data_dir, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for x0, x1, y in zip(train_data[0], train_data[1], train_data[2]):\n",
    "    max_length = max(max_length, len(x0.split()), len(x1.split()))\n",
    "    \n",
    "for x0, x1, y in zip(test_data[0], test_data[1], test_data[2]):\n",
    "    max_length = max(max_length, len(x0.split()), len(x1.split()))\n",
    "\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集有大约55万组，测试集有大约10000组。结果表明，在训练集和测试集中，“蕴涵”、“矛盾”和“中性”三个标签是平衡的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 16674\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    \n",
    "    return [tokenizer(review) for review in data]\n",
    "\n",
    "def get_vocab(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized(data[0]+data[1])\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab(train_data+test_data)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = max_length  # 将每条评论通过截断或者补0，使得长度变成max_length\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data_x0 = get_tokenized(data[0])\n",
    "    features_x0 = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data_x0])\n",
    "    \n",
    "    tokenized_data_x1 = get_tokenized(data[1])\n",
    "    features_x1 = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data_x0])\n",
    "    labels = torch.tensor([score for score in data[2]])\n",
    "    return features_x0, features_x1, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 torch.Size([128, 82]) x1 torch.Size([128, 82]) y torch.Size([128])\n",
      "#batches: 4292\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess(test_data, vocab))\n",
    "\n",
    "batch_size = 128\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for x0, x1, y in train_iter:\n",
    "    print('x0', x0.shape, 'x1', x1.shape, 'y', y.shape)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for labels, batch in enumerate(test_iter):\n",
    "#     print(len(batch[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x0, x1, y in train_iter:\n",
    "#     print('x0', x0, 'x1', x1.shape, 'y', y.shape)\n",
    "#     embeds = nn.Embedding(len(vocab), 100)\n",
    "# #     print(embeds(x0))\n",
    "#     print((embeds(x0).transpose(1, 2).contiguous()).transpose(1, 2).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            num_hiddens: 隐藏状态维度大小\n",
    "            num_layers: 隐藏层个数\n",
    "        '''\n",
    "        super(ESIM, self).__init__()\n",
    "        self.dropout = 0.2\n",
    "        self.hidden_size = num_hiddens\n",
    "        self.embeds_dim = embed_size\n",
    "        self.num_layers = num_layers\n",
    "                 \n",
    "        self.embeds = nn.Embedding(len(vocab), self.embeds_dim)\n",
    "        self.bn_embeds = nn.BatchNorm1d(self.embeds_dim)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=self.embeds_dim, \n",
    "                             hidden_size=self.hidden_size,\n",
    "                             num_layers=self.num_layers,\n",
    "                             bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_size * 8, \n",
    "                             hidden_size=self.hidden_size, \n",
    "                             num_layers=self.num_layers,\n",
    "                             bidirectional=True)\n",
    " \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_size * 8),\n",
    "            nn.Linear(self.hidden_size * 8, 16),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    " \n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        '''\n",
    "        x1: batch_size * seq_len * dim\n",
    "        x2: batch_size * seq_len * dim\n",
    "        '''\n",
    "        # attention: batch_size * seq_len * seq_len\n",
    "        attention = torch.matmul(x1, x2.transpose(1, 2))\n",
    "        # mask1 = mask1.float().masked_fill_(mask1, float('-inf'))\n",
    "        # mask2 = mask2.float().masked_fill_(mask2, float('-inf'))\n",
    " \n",
    "        # weight: batch_size * seq_len * seq_len\n",
    "        # weight1 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)\n",
    "        weight1 = F.softmax(attention, dim=-1)\n",
    "        x1_align = torch.matmul(weight1, x2)\n",
    "        # weight2 = F.softmax(attention.transpose(1, 2) + mask1.unsqueeze(1), dim=-1)\n",
    "        weight2 = F.softmax(attention.transpose(1, 2), dim=-1)\n",
    "        x2_align = torch.matmul(weight2, x1)\n",
    "        # x_align: batch_size * seq_len * hidden_size\n",
    " \n",
    "        return x1_align, x2_align\n",
    " \n",
    "    def submul(self, x1, x2):\n",
    "        mul = x1 * x2\n",
    "        sub = x1 - x2\n",
    "        return torch.cat([sub, mul], -1)\n",
    " \n",
    "    def apply_multiple(self, x):\n",
    "        # input: batch_size * seq_len * (2 * hidden_size)\n",
    "        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        # output: batch_size * (4 * hidden_size)\n",
    "        return torch.cat([p1, p2], 1)\n",
    " \n",
    "    def forward(self, *input):\n",
    "        # batch_size * seq_len\n",
    "        sent1, sent2 = input[0], input[1]\n",
    "#         print(sent1.shape, sent2.shape)\n",
    "        mask1, mask2 = sent1.eq(0), sent2.eq(0)\n",
    " \n",
    "        # embeds: batch_size * seq_len => batch_size * seq_len * dim\n",
    "        x1 = self.bn_embeds(self.embeds(sent1).transpose(1, 2).contiguous()).transpose(1, 2)\n",
    "        x2 = self.bn_embeds(self.embeds(sent2).transpose(1, 2).contiguous()).transpose(1, 2)\n",
    " \n",
    "        # batch_size * seq_len * dim =>      batch_size * seq_len * hidden_size\n",
    "        o1, _ = self.lstm1(x1)\n",
    "        o2, _ = self.lstm1(x2)\n",
    " \n",
    "        # Attention\n",
    "        # batch_size * seq_len * hidden_size\n",
    "        q1_align, q2_align = self.soft_attention_align(o1, o2, mask1, mask2)\n",
    " \n",
    "        # Compose\n",
    "        # batch_size * seq_len * (8 * hidden_size)\n",
    "        q1_combined = torch.cat([o1, q1_align, self.submul(o1, q1_align)], -1)\n",
    "        q2_combined = torch.cat([o2, q2_align, self.submul(o2, q2_align)], -1)\n",
    " \n",
    "        # batch_size * seq_len * (2 * hidden_size)\n",
    "        q1_compose, _ = self.lstm2(q1_combined)\n",
    "        q2_compose, _ = self.lstm2(q2_combined)\n",
    " \n",
    "        # Aggregate\n",
    "        # input: batch_size * seq_len * (2 * hidden_size)\n",
    "        # output: batch_size * (4 * hidden_size)\n",
    "        q1_rep = self.apply_multiple(q1_compose)\n",
    "        q2_rep = self.apply_multiple(q2_compose)\n",
    " \n",
    "        # Classifier\n",
    "        x = torch.cat([q1_rep, q2_rep], -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = ESIM(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_dir = \"./\"\n",
    "# glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n",
    "\n",
    "# def load_pretrained_embedding(words, pretrained_vocab):\n",
    "#     '''\n",
    "#     @params:\n",
    "#         words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "#         pretrained_vocab: 预训练词向量\n",
    "#     @return:\n",
    "#         embed: 加载到的词向量\n",
    "#     '''\n",
    "#     embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "#     oov_count = 0 # out of vocabulary\n",
    "#     for i, word in enumerate(words):\n",
    "#         try:\n",
    "#             idx = pretrained_vocab.stoi[word]\n",
    "#             embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "#         except KeyError:\n",
    "#             oov_count += 1\n",
    "#     if oov_count > 0:\n",
    "#         print(\"There are %d oov words.\" % oov_count)\n",
    "#     return embed\n",
    "\n",
    "# net.embeds.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2, 3),)\n"
     ]
    }
   ],
   "source": [
    "def func(*args):\n",
    "    print(args)\n",
    "q = 1, 2, 3\n",
    "func(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x0, x1, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(x0.to(device), x1.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(x0, x1, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(x0, x1).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for x0, x1, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x0 = x0.to(device)\n",
    "            x1 = x1.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = net(x0,x1)\n",
    "#             print((y_hat.argmax(dim=1) == y).float().sum()/len(y))\n",
    "            l = loss(y_hat, y)\n",
    "            \n",
    "            \n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.1012, train acc 0.333, test acc 0.331, time 1223.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.0005, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
