{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4) (66292, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./train.tsv.zip', sep=\"\\t\")\n",
    "test = pd.read_csv('./test.tsv.zip', sep=\"\\t\")\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_num = train['Sentiment'].max() + 1\n",
    "y_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 复现2-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在task1中我们已经看到N-gram模型因为采集了一定的语序信息要优于词袋模型，因此这里采用pytorch复现2-gram。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入停用词库\n",
    "file = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "try:\n",
    "    while True:\n",
    "        text_line = file.readline()\n",
    "        if text_line:\n",
    "            stopwords.append(text_line.strip())\n",
    "        else:\n",
    "            break\n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'d' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dic = {} # 词库\n",
    "dic_length = 0\n",
    "# 训练集\n",
    "for i in train.index:\n",
    "    words = train['Phrase'][i].split(' ')\n",
    "    words_ = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            words_ =  words_ + \" \" + word\n",
    "    words = words_.strip().split()\n",
    "    words_length = len(words)\n",
    "    for j in range(words_length-1):\n",
    "        word = words[j] + \" \" + words[j+1]\n",
    "        if word not in words_dic.keys():\n",
    "            words_dic[word] = dic_length\n",
    "            dic_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63405"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A series': 0,\n",
       " 'series escapades': 1,\n",
       " 'escapades demonstrating': 2,\n",
       " 'demonstrating adage': 3,\n",
       " 'adage goose': 4,\n",
       " 'goose gander': 5,\n",
       " 'gander ,': 6,\n",
       " ', occasionally': 7,\n",
       " 'occasionally amuses': 8,\n",
       " 'amuses amounts': 9,\n",
       " 'amounts story': 10,\n",
       " 'story .': 11,\n",
       " 'This quiet': 12,\n",
       " 'quiet ,': 13,\n",
       " ', introspective': 14,\n",
       " 'introspective entertaining': 15,\n",
       " 'entertaining independent': 16,\n",
       " 'independent worth': 17,\n",
       " 'worth seeking': 18,\n",
       " 'seeking .': 19,\n",
       " 'Even fans': 20,\n",
       " 'fans Ismail': 21,\n",
       " 'Ismail Merchant': 22,\n",
       " 'Merchant ,': 23,\n",
       " ', I': 24,\n",
       " 'I suspect': 25,\n",
       " 'suspect ,': 26,\n",
       " ', hard': 27,\n",
       " 'hard time': 28,\n",
       " 'time sitting': 29,\n",
       " 'sitting .': 30,\n",
       " 'A positively': 31,\n",
       " 'positively thrilling': 32,\n",
       " 'thrilling combination': 33,\n",
       " 'combination ethnography': 34,\n",
       " 'ethnography intrigue': 35,\n",
       " 'intrigue ,': 36,\n",
       " ', betrayal': 37,\n",
       " 'betrayal ,': 38,\n",
       " ', deceit': 39,\n",
       " 'deceit murder': 40,\n",
       " 'murder Shakespearean': 41,\n",
       " 'Shakespearean tragedy': 42,\n",
       " 'tragedy juicy': 43,\n",
       " 'juicy soap': 44,\n",
       " 'soap opera': 45,\n",
       " 'opera .': 46,\n",
       " 'Aggressive self-glorification': 47,\n",
       " 'self-glorification manipulative': 48,\n",
       " 'manipulative whitewash': 49,\n",
       " 'whitewash .': 50,\n",
       " 'A comedy-drama': 51,\n",
       " 'comedy-drama epic': 52,\n",
       " 'epic proportions': 53,\n",
       " 'proportions rooted': 54,\n",
       " 'rooted sincere': 55,\n",
       " 'sincere performance': 56,\n",
       " 'performance title': 57,\n",
       " 'title character': 58,\n",
       " 'character undergoing': 59,\n",
       " 'undergoing midlife': 60,\n",
       " 'midlife crisis': 61,\n",
       " 'crisis .': 62,\n",
       " 'Narratively ,': 63,\n",
       " ', Trouble': 64,\n",
       " 'Trouble Every': 65,\n",
       " 'Every Day': 66,\n",
       " 'Day plodding': 67,\n",
       " 'plodding mess': 68,\n",
       " 'mess .': 69,\n",
       " 'The Importance': 70,\n",
       " 'Importance Being': 71,\n",
       " 'Being Earnest': 72,\n",
       " 'Earnest ,': 73,\n",
       " ', thick': 74,\n",
       " 'thick wit': 75,\n",
       " 'wit plays': 76,\n",
       " 'plays reading': 77,\n",
       " 'reading Bartlett': 78,\n",
       " 'Bartlett Familiar': 79,\n",
       " 'Familiar Quotations': 80,\n",
       " 'But leave': 81,\n",
       " 'leave .': 82,\n",
       " 'You hate': 83,\n",
       " 'hate reason': 84,\n",
       " 'reason .': 85,\n",
       " 'There recommend': 86,\n",
       " 'recommend Snow': 87,\n",
       " 'Snow Dogs': 88,\n",
       " 'Dogs ,': 89,\n",
       " ', considers': 90,\n",
       " 'considers cliched': 91,\n",
       " 'cliched dialogue': 92,\n",
       " 'dialogue perverse': 93,\n",
       " 'perverse escapism': 94,\n",
       " 'escapism source': 95,\n",
       " 'source hilarity': 96,\n",
       " 'hilarity .': 97,\n",
       " 'Kung Pow': 98,\n",
       " 'Pow Oedekerk': 99,\n",
       " 'Oedekerk realization': 100,\n",
       " 'realization childhood': 101,\n",
       " 'childhood dream': 102,\n",
       " 'dream martial-arts': 103,\n",
       " 'martial-arts flick': 104,\n",
       " 'flick ,': 105,\n",
       " ', proves': 106,\n",
       " 'proves dreams': 107,\n",
       " 'dreams youth': 108,\n",
       " 'youth remain': 109,\n",
       " 'remain .': 110,\n",
       " 'The performances': 111,\n",
       " 'performances absolute': 112,\n",
       " 'absolute joy': 113,\n",
       " 'joy .': 114,\n",
       " 'Fresnadillo extravagant': 115,\n",
       " 'extravagant chance': 116,\n",
       " 'chance distort': 117,\n",
       " 'distort perspective': 118,\n",
       " 'perspective throw': 119,\n",
       " 'throw path': 120,\n",
       " 'path sense': 121,\n",
       " 'sense .': 122,\n",
       " 'I Moonlight': 123,\n",
       " 'Moonlight Mile': 124,\n",
       " 'Mile ,': 125,\n",
       " ', judgment': 126,\n",
       " 'judgment damned': 127,\n",
       " 'damned .': 128,\n",
       " 'A relief': 129,\n",
       " 'relief baseball': 130,\n",
       " 'baseball movies': 131,\n",
       " 'movies hard': 132,\n",
       " 'hard mythic': 133,\n",
       " 'mythic ,': 134,\n",
       " ', sweet': 135,\n",
       " 'sweet modest': 136,\n",
       " 'modest ultimately': 137,\n",
       " 'ultimately winning': 138,\n",
       " 'winning story': 139,\n",
       " 'bilingual charmer': 140,\n",
       " 'charmer ,': 141,\n",
       " ', woman': 142,\n",
       " 'woman inspired': 143,\n",
       " 'Like dizzily': 144,\n",
       " 'dizzily gorgeous': 145,\n",
       " 'gorgeous companion': 146,\n",
       " 'companion Mr.': 147,\n",
       " 'Mr. Wong': 148,\n",
       " 'Wong In': 149,\n",
       " 'In Mood': 150,\n",
       " 'Mood Love': 151,\n",
       " 'Love --': 152,\n",
       " '-- Hong': 153,\n",
       " 'Hong Kong': 154,\n",
       " 'Kong movie': 155,\n",
       " 'movie mainland': 156,\n",
       " 'mainland setting': 157,\n",
       " 'setting .': 158,\n",
       " 'As inept': 159,\n",
       " 'inept big-screen': 160,\n",
       " 'big-screen remakes': 161,\n",
       " 'remakes The': 162,\n",
       " 'The Avengers': 163,\n",
       " 'Avengers The': 164,\n",
       " 'The Wild': 165,\n",
       " 'Wild Wild': 166,\n",
       " 'Wild West': 167,\n",
       " 'West .': 168,\n",
       " 'It expect': 169,\n",
       " 'expect --': 170,\n",
       " '-- .': 171,\n",
       " 'Best indie': 172,\n",
       " 'indie ,': 173,\n",
       " ', .': 174,\n",
       " 'Hatfield Hicks': 175,\n",
       " 'Hicks oddest': 176,\n",
       " 'oddest couples': 177,\n",
       " 'couples ,': 178,\n",
       " ', sense': 179,\n",
       " 'sense movie': 180,\n",
       " 'movie study': 181,\n",
       " 'study gambles': 182,\n",
       " 'gambles publishing': 183,\n",
       " 'publishing ,': 184,\n",
       " ', offering': 185,\n",
       " 'offering study': 186,\n",
       " 'study exists': 187,\n",
       " 'exists movie': 188,\n",
       " 'movie political': 189,\n",
       " 'political ramifications': 190,\n",
       " 'ramifications .': 191,\n",
       " 'It house': 192,\n",
       " 'house party': 193,\n",
       " 'party watching': 194,\n",
       " 'watching host': 195,\n",
       " 'host defend': 196,\n",
       " 'defend frothing': 197,\n",
       " 'frothing ex-girlfriend': 198,\n",
       " 'ex-girlfriend .': 199,\n",
       " 'That Chuck': 200,\n",
       " 'Chuck Norris': 201,\n",
       " 'Norris ``': 202,\n",
       " '`` grenade': 203,\n",
       " 'grenade gag': 204,\n",
       " \"gag ''\": 205,\n",
       " \"'' occurs\": 206,\n",
       " 'occurs 7': 207,\n",
       " '7 times': 208,\n",
       " 'times Windtalkers': 209,\n",
       " 'Windtalkers indication': 210,\n",
       " 'indication serious-minded': 211,\n",
       " 'serious-minded film': 212,\n",
       " 'film .': 213,\n",
       " 'The plot': 214,\n",
       " 'plot romantic': 215,\n",
       " 'romantic comedy': 216,\n",
       " 'comedy boilerplate': 217,\n",
       " 'boilerplate start': 218,\n",
       " 'start finish': 219,\n",
       " 'finish .': 220,\n",
       " 'It arrives': 221,\n",
       " 'arrives impeccable': 222,\n",
       " 'impeccable pedigree': 223,\n",
       " 'pedigree ,': 224,\n",
       " ', mongrel': 225,\n",
       " 'mongrel pep': 226,\n",
       " 'pep ,': 227,\n",
       " ', indecipherable': 228,\n",
       " 'indecipherable plot': 229,\n",
       " 'plot complications': 230,\n",
       " 'complications .': 231,\n",
       " 'A film': 232,\n",
       " 'film preach': 233,\n",
       " 'preach exclusively': 234,\n",
       " 'exclusively converted': 235,\n",
       " 'converted .': 236,\n",
       " 'While The': 237,\n",
       " 'Earnest offers': 238,\n",
       " 'offers opportunities': 239,\n",
       " 'opportunities occasional': 240,\n",
       " 'occasional smiles': 241,\n",
       " 'smiles chuckles': 242,\n",
       " 'chuckles ,': 243,\n",
       " ', reason': 244,\n",
       " 'reason theater': 245,\n",
       " 'theater Wilde': 246,\n",
       " 'Wilde wit': 247,\n",
       " 'wit actors': 248,\n",
       " \"actors '\": 249,\n",
       " \"' performances\": 250,\n",
       " 'performances .': 251,\n",
       " 'The vapid': 252,\n",
       " 'vapid actor': 253,\n",
       " 'actor exercise': 254,\n",
       " 'exercise structure': 255,\n",
       " 'structure Arthur': 256,\n",
       " 'Arthur Schnitzler': 257,\n",
       " 'Schnitzler Reigen': 258,\n",
       " 'Reigen .': 259,\n",
       " 'More vaudeville': 260,\n",
       " 'vaudeville well-constructed': 261,\n",
       " 'well-constructed narrative': 262,\n",
       " 'narrative ,': 263,\n",
       " ', terms': 264,\n",
       " 'terms inoffensive': 265,\n",
       " 'inoffensive sweet': 266,\n",
       " 'sweet .': 267,\n",
       " 'Nothing run-of-the-mill': 268,\n",
       " 'run-of-the-mill action': 269,\n",
       " 'action flick': 270,\n",
       " 'flick .': 271,\n",
       " 'Hampered --': 272,\n",
       " '-- ,': 273,\n",
       " ', paralyzed': 274,\n",
       " 'paralyzed --': 275,\n",
       " '-- self-indulgent': 276,\n",
       " 'self-indulgent script': 277,\n",
       " 'script ...': 278,\n",
       " '... aims': 279,\n",
       " 'aims poetry': 280,\n",
       " 'poetry sounding': 281,\n",
       " 'sounding satire': 282,\n",
       " 'satire .': 283,\n",
       " 'Ice Age': 284,\n",
       " 'Age computer-generated': 285,\n",
       " 'computer-generated feature': 286,\n",
       " 'feature cartoon': 287,\n",
       " 'cartoon feel': 288,\n",
       " 'feel movies': 289,\n",
       " 'movies ,': 290,\n",
       " ', glacial': 291,\n",
       " 'glacial pacing': 292,\n",
       " 'pacing .': 293,\n",
       " 'There sense': 294,\n",
       " 'sense ,': 295,\n",
       " ', makers': 296,\n",
       " 'makers serve': 297,\n",
       " 'serve cliches': 298,\n",
       " 'cliches considerable': 299,\n",
       " 'considerable dash': 300,\n",
       " 'dash .': 301,\n",
       " 'Cattaneo runaway': 302,\n",
       " 'runaway success': 303,\n",
       " 'success film': 304,\n",
       " 'film ,': 305,\n",
       " ', The': 306,\n",
       " 'The Full': 307,\n",
       " 'Full Monty': 308,\n",
       " 'Monty ,': 309,\n",
       " 'They unnamed': 310,\n",
       " 'unnamed ,': 311,\n",
       " ', easily': 312,\n",
       " 'easily substitutable': 313,\n",
       " 'substitutable forces': 314,\n",
       " 'forces serve': 315,\n",
       " 'serve terror': 316,\n",
       " 'terror heroes': 317,\n",
       " 'heroes horror': 318,\n",
       " 'horror movies': 319,\n",
       " 'movies avoid': 320,\n",
       " 'avoid .': 321,\n",
       " 'It feels': 322,\n",
       " 'feels movie': 323,\n",
       " 'movie entertaining': 324,\n",
       " 'entertaining amusing': 325,\n",
       " 'amusing .': 326,\n",
       " 'The movie': 327,\n",
       " 'movie progression': 328,\n",
       " 'progression rambling': 329,\n",
       " 'rambling incoherence': 330,\n",
       " 'incoherence meaning': 331,\n",
       " 'meaning phrase': 332,\n",
       " 'phrase `': 333,\n",
       " '` fatal': 334,\n",
       " 'fatal script': 335,\n",
       " 'script error': 336,\n",
       " 'error .': 337,\n",
       " \". '\": 338,\n",
       " 'Do judge': 339,\n",
       " 'judge -': 340,\n",
       " '- dark': 341,\n",
       " 'dark ,': 342,\n",
       " ', gritty': 343,\n",
       " 'gritty story': 344,\n",
       " 'story takes': 345,\n",
       " 'takes totally': 346,\n",
       " 'totally unexpected': 347,\n",
       " 'unexpected directions': 348,\n",
       " 'directions .': 349,\n",
       " 'So romantics': 350,\n",
       " 'romantics .': 351,\n",
       " 'Tartakovsky team': 352,\n",
       " 'team freakish': 353,\n",
       " 'freakish powers': 354,\n",
       " 'powers visual': 355,\n",
       " 'visual charm': 356,\n",
       " 'charm ,': 357,\n",
       " ', writers': 358,\n",
       " 'writers slip': 359,\n",
       " 'slip modern': 360,\n",
       " 'modern rut': 361,\n",
       " 'rut narrative': 362,\n",
       " 'narrative banality': 363,\n",
       " 'banality .': 364,\n",
       " 'Vincent Gallo': 365,\n",
       " 'Gallo French': 366,\n",
       " 'French shocker': 367,\n",
       " 'shocker playing': 368,\n",
       " 'playing usual': 369,\n",
       " 'usual bad': 370,\n",
       " 'bad boy': 371,\n",
       " 'boy weirdo': 372,\n",
       " 'weirdo role': 373,\n",
       " 'role .': 374,\n",
       " 'If horror': 375,\n",
       " 'horror movie': 376,\n",
       " 'movie primary': 377,\n",
       " 'primary goal': 378,\n",
       " 'goal frighten': 379,\n",
       " 'frighten disturb': 380,\n",
       " 'disturb ,': 381,\n",
       " ', They': 382,\n",
       " 'They spectacularly': 383,\n",
       " 'spectacularly ...': 384,\n",
       " '... A': 385,\n",
       " 'A shiver-inducing': 386,\n",
       " 'shiver-inducing ,': 387,\n",
       " ', nerve-rattling': 388,\n",
       " 'nerve-rattling ride': 389,\n",
       " 'ride .': 390,\n",
       " 'This 100-minute': 391,\n",
       " '100-minute movie': 392,\n",
       " 'movie 25': 393,\n",
       " '25 minutes': 394,\n",
       " 'minutes decent': 395,\n",
       " 'decent material': 396,\n",
       " 'material .': 397,\n",
       " 'Fortunately ,': 398,\n",
       " ', option': 399,\n",
       " 'option .': 400,\n",
       " 'Less sensational': 401,\n",
       " 'sensational true-crime': 402,\n",
       " 'true-crime hell-jaunt': 403,\n",
       " 'hell-jaunt purists': 404,\n",
       " 'purists experimental': 405,\n",
       " 'experimental storytelling': 406,\n",
       " 'storytelling -LRB-': 407,\n",
       " '-LRB- horrifying': 408,\n",
       " 'horrifying -RRB-': 409,\n",
       " '-RRB- .': 410,\n",
       " 'As tricky': 411,\n",
       " 'tricky satisfying': 412,\n",
       " 'satisfying David': 413,\n",
       " 'David Mamet': 414,\n",
       " 'Mamet airless': 415,\n",
       " 'airless cinematic': 416,\n",
       " 'cinematic shell': 417,\n",
       " 'shell games': 418,\n",
       " 'games .': 419,\n",
       " 'I filmmaker': 420,\n",
       " 'filmmaker disagree': 421,\n",
       " 'disagree ,': 422,\n",
       " ', ,': 423,\n",
       " ', honestly': 424,\n",
       " 'honestly ,': 425,\n",
       " 'I .': 426,\n",
       " 'Jones tackled': 427,\n",
       " 'tackled meaty': 428,\n",
       " 'meaty subject': 429,\n",
       " 'subject drawn': 430,\n",
       " 'drawn engaging': 431,\n",
       " 'engaging characters': 432,\n",
       " 'characters peppering': 433,\n",
       " 'peppering memorable': 434,\n",
       " 'memorable zingers': 435,\n",
       " 'zingers .': 436,\n",
       " 'Bloody Sunday': 437,\n",
       " 'Sunday grace': 438,\n",
       " 'grace call': 439,\n",
       " 'call prevention': 440,\n",
       " 'prevention blame': 441,\n",
       " 'blame ,': 442,\n",
       " ', war': 443,\n",
       " 'war movies': 444,\n",
       " 'movies .': 445,\n",
       " 'Takes clunky': 446,\n",
       " 'clunky TV-movie': 447,\n",
       " 'TV-movie approach': 448,\n",
       " 'approach detailing': 449,\n",
       " 'detailing chapter': 450,\n",
       " 'chapter life': 451,\n",
       " 'life celebrated': 452,\n",
       " 'celebrated Irish': 453,\n",
       " 'Irish playwright': 454,\n",
       " 'playwright ,': 455,\n",
       " ', poet': 456,\n",
       " 'poet drinker': 457,\n",
       " 'drinker .': 458,\n",
       " 'Finally coming': 459,\n",
       " 'coming Miramax': 460,\n",
       " 'Miramax deep': 461,\n",
       " 'deep shelves': 462,\n",
       " 'shelves couple': 463,\n",
       " 'couple aborted': 464,\n",
       " 'aborted attempts': 465,\n",
       " 'attempts ,': 466,\n",
       " ', Waking': 467,\n",
       " 'Waking Up': 468,\n",
       " 'Up Reno': 469,\n",
       " 'Reno strong': 470,\n",
       " 'strong letting': 471,\n",
       " 'letting sleeping': 472,\n",
       " 'sleeping dogs': 473,\n",
       " 'dogs lie': 474,\n",
       " 'lie .': 475,\n",
       " 'Thanks Williams': 476,\n",
       " 'Williams ,': 477,\n",
       " ', developments': 478,\n",
       " 'developments processed': 479,\n",
       " 'processed 60': 480,\n",
       " '60 minutes': 481,\n",
       " 'minutes --': 482,\n",
       " '-- rest': 483,\n",
       " 'rest overexposed': 484,\n",
       " 'overexposed waste': 485,\n",
       " 'waste film': 486,\n",
       " 'Comes relic': 487,\n",
       " 'relic bygone': 488,\n",
       " 'bygone era': 489,\n",
       " 'era ,': 490,\n",
       " ', convolutions': 491,\n",
       " 'convolutions ...': 492,\n",
       " '... feel': 493,\n",
       " 'feel silly': 494,\n",
       " 'silly plausible': 495,\n",
       " 'plausible .': 496,\n",
       " 'Perhaps cliche': 497,\n",
       " 'cliche call': 498,\n",
       " 'call film': 499,\n",
       " 'film `': 500,\n",
       " '` refreshing': 501,\n",
       " 'refreshing ,': 502,\n",
       " \", '\": 503,\n",
       " \"' .\": 504,\n",
       " 'Both lead': 505,\n",
       " 'lead performances': 506,\n",
       " 'performances Oscar-size': 507,\n",
       " 'Oscar-size .': 508,\n",
       " 'Run lives': 509,\n",
       " 'lives !': 510,\n",
       " 'Mark Pellington': 511,\n",
       " 'Pellington pop': 512,\n",
       " 'pop thriller': 513,\n",
       " 'thriller kooky': 514,\n",
       " 'kooky overeager': 515,\n",
       " 'overeager spooky': 516,\n",
       " 'spooky subtly': 517,\n",
       " 'subtly love': 518,\n",
       " 'love myth': 519,\n",
       " 'myth .': 520,\n",
       " 'Claude Chabrol': 521,\n",
       " 'Chabrol camera': 522,\n",
       " 'camera gently': 523,\n",
       " 'gently swaying': 524,\n",
       " 'swaying cradles': 525,\n",
       " 'cradles characters': 526,\n",
       " 'characters ,': 527,\n",
       " ', veiling': 528,\n",
       " 'veiling tension': 529,\n",
       " 'tension beneath': 530,\n",
       " 'beneath tender': 531,\n",
       " 'tender movements': 532,\n",
       " 'movements .': 533,\n",
       " 'Transforms -LRB-': 534,\n",
       " '-LRB- Shakespeare': 535,\n",
       " 'Shakespeare -RRB-': 536,\n",
       " '-RRB- deepest': 537,\n",
       " 'deepest tragedies': 538,\n",
       " 'tragedies smart': 539,\n",
       " 'smart comedy': 540,\n",
       " 'comedy .': 541,\n",
       " 'The screenplay': 542,\n",
       " 'screenplay James': 543,\n",
       " 'James Eric': 544,\n",
       " 'Eric ,': 545,\n",
       " ', James': 546,\n",
       " 'James Horton': 547,\n",
       " 'Horton director': 548,\n",
       " 'director Peter': 549,\n",
       " \"Peter O'Fallon\": 550,\n",
       " \"O'Fallon ...\": 551,\n",
       " '... pat': 552,\n",
       " 'pat teeth': 553,\n",
       " 'teeth hurt': 554,\n",
       " 'hurt .': 555,\n",
       " 'But arriving': 556,\n",
       " 'arriving dark': 557,\n",
       " 'dark moment': 558,\n",
       " 'moment history': 559,\n",
       " 'history ,': 560,\n",
       " ', offers': 561,\n",
       " 'offers flickering': 562,\n",
       " 'flickering reminders': 563,\n",
       " 'reminders ties': 564,\n",
       " 'ties bind': 565,\n",
       " 'bind .': 566,\n",
       " 'Its generic': 567,\n",
       " 'generic villains': 568,\n",
       " 'villains lack': 569,\n",
       " 'lack intrigue': 570,\n",
       " 'intrigue -LRB-': 571,\n",
       " '-LRB- funny': 572,\n",
       " 'funny accents': 573,\n",
       " 'accents -RRB-': 574,\n",
       " '-RRB- action': 575,\n",
       " 'action scenes': 576,\n",
       " 'scenes delivered': 577,\n",
       " 'delivered .': 578,\n",
       " 'The characters': 579,\n",
       " 'characters deeply': 580,\n",
       " 'deeply `': 581,\n",
       " '` right-thinking': 582,\n",
       " \"right-thinking '\": 583,\n",
       " \"' films\": 584,\n",
       " 'films .': 585,\n",
       " 'The film': 586,\n",
       " ', gratuitous': 587,\n",
       " 'gratuitous cinematic': 588,\n",
       " 'cinematic distractions': 589,\n",
       " 'distractions impressed': 590,\n",
       " 'impressed ,': 591,\n",
       " ', fun': 592,\n",
       " 'fun .': 593,\n",
       " 'Downright transparent': 594,\n",
       " 'transparent script': 595,\n",
       " 'script endless': 596,\n",
       " 'endless assault': 597,\n",
       " 'assault embarrassingly': 598,\n",
       " 'embarrassingly ham-fisted': 599,\n",
       " 'ham-fisted sex': 600,\n",
       " 'sex jokes': 601,\n",
       " 'jokes reek': 602,\n",
       " 'reek script': 603,\n",
       " 'script rewrite': 604,\n",
       " 'rewrite designed': 605,\n",
       " 'designed garner': 606,\n",
       " 'garner film': 607,\n",
       " 'film ``': 608,\n",
       " '`` cooler': 609,\n",
       " \"cooler ''\": 610,\n",
       " \"'' PG-13\": 611,\n",
       " 'PG-13 rating': 612,\n",
       " 'rating .': 613,\n",
       " 'A smart': 614,\n",
       " 'smart ,': 615,\n",
       " ', provocative': 616,\n",
       " 'provocative drama': 617,\n",
       " 'drama impossible': 618,\n",
       " 'impossible :': 619,\n",
       " ': It': 620,\n",
       " 'It skin': 621,\n",
       " 'skin evil': 622,\n",
       " 'evil ,': 623,\n",
       " ', monstrous': 624,\n",
       " 'monstrous lunatic': 625,\n",
       " 'lunatic .': 626,\n",
       " 'It drive-by': 627,\n",
       " 'drive-by .': 628,\n",
       " '-- stoner': 629,\n",
       " 'stoner midnight': 630,\n",
       " 'midnight flick': 631,\n",
       " ', sci-fi': 632,\n",
       " 'sci-fi deconstruction': 633,\n",
       " 'deconstruction ,': 634,\n",
       " ', gay': 635,\n",
       " 'gay fantasia': 636,\n",
       " 'fantasia --': 637,\n",
       " '-- love': 638,\n",
       " 'love story': 639,\n",
       " 'story sanguine': 640,\n",
       " 'sanguine title': 641,\n",
       " 'title .': 642,\n",
       " 'A funny': 643,\n",
       " 'funny romantic': 644,\n",
       " 'comedy skittish': 645,\n",
       " 'skittish New': 646,\n",
       " 'New York': 647,\n",
       " 'York middle-agers': 648,\n",
       " 'middle-agers stumble': 649,\n",
       " 'stumble relationship': 650,\n",
       " 'relationship struggle': 651,\n",
       " 'struggle furiously': 652,\n",
       " 'furiously fears': 653,\n",
       " 'fears foibles': 654,\n",
       " 'foibles .': 655,\n",
       " 'But ambition': 656,\n",
       " 'ambition subjects': 657,\n",
       " 'subjects ,': 658,\n",
       " ', willingness': 659,\n",
       " 'willingness .': 660,\n",
       " '-LRB- Scherfig': 661,\n",
       " 'Scherfig -RRB-': 662,\n",
       " '-RRB- movie': 663,\n",
       " 'movie leave': 664,\n",
       " 'leave wondering': 665,\n",
       " 'wondering characters': 666,\n",
       " \"characters '\": 667,\n",
       " \"' lives\": 668,\n",
       " 'lives clever': 669,\n",
       " 'clever credits': 670,\n",
       " 'credits roll': 671,\n",
       " 'roll .': 672,\n",
       " '-LRB- An': 673,\n",
       " 'An -RRB-': 674,\n",
       " '-RRB- absorbing': 675,\n",
       " 'absorbing documentary': 676,\n",
       " 'documentary .': 677,\n",
       " 'Reeks rot': 678,\n",
       " 'rot hack': 679,\n",
       " 'hack start': 680,\n",
       " 'Plays series': 681,\n",
       " 'series vignettes': 682,\n",
       " 'vignettes --': 683,\n",
       " '-- clips': 684,\n",
       " 'clips film': 685,\n",
       " 'film common': 686,\n",
       " 'common through-line': 687,\n",
       " 'through-line .': 688,\n",
       " 'Contrived pastiche': 689,\n",
       " 'pastiche caper': 690,\n",
       " 'The major': 691,\n",
       " 'major Windtalkers': 692,\n",
       " 'Windtalkers bulk': 693,\n",
       " 'bulk movie': 694,\n",
       " 'movie centers': 695,\n",
       " 'centers wrong': 696,\n",
       " 'wrong character': 697,\n",
       " 'character .': 698,\n",
       " 'Swimfan ,': 699,\n",
       " ', Fatal': 700,\n",
       " 'Fatal Attraction': 701,\n",
       " 'Attraction ,': 702,\n",
       " ', eventually': 703,\n",
       " 'eventually overboard': 704,\n",
       " 'overboard loony': 705,\n",
       " 'loony melodramatic': 706,\n",
       " 'melodramatic denouement': 707,\n",
       " 'denouement school': 708,\n",
       " 'school swimming': 709,\n",
       " 'swimming pool': 710,\n",
       " 'pool substitutes': 711,\n",
       " 'substitutes bathtub': 712,\n",
       " 'bathtub .': 713,\n",
       " 'This heartfelt': 714,\n",
       " 'heartfelt story': 715,\n",
       " 'story ...': 716,\n",
       " '... involving': 717,\n",
       " 'involving .': 718,\n",
       " 'An unsettling': 719,\n",
       " 'unsettling ,': 720,\n",
       " ', memorable': 721,\n",
       " 'memorable cinematic': 722,\n",
       " 'cinematic experience': 723,\n",
       " 'experience predecessors': 724,\n",
       " 'predecessors .': 725,\n",
       " 'All ,': 726,\n",
       " ', pretty': 727,\n",
       " 'pretty execution': 728,\n",
       " 'execution story': 729,\n",
       " 'story lot': 730,\n",
       " 'lot richer': 731,\n",
       " 'richer Hollywood': 732,\n",
       " 'Hollywood action': 733,\n",
       " 'action screenwriters': 734,\n",
       " 'screenwriters .': 735,\n",
       " 'Phillip Noyce': 736,\n",
       " 'Noyce actors': 737,\n",
       " 'actors --': 738,\n",
       " '-- cinematographer': 739,\n",
       " 'cinematographer ,': 740,\n",
       " ', Christopher': 741,\n",
       " 'Christopher Doyle': 742,\n",
       " 'Doyle --': 743,\n",
       " '-- understand': 744,\n",
       " 'understand delicate': 745,\n",
       " 'delicate forcefulness': 746,\n",
       " 'forcefulness Greene': 747,\n",
       " 'Greene prose': 748,\n",
       " 'prose ,': 749,\n",
       " ', screen': 750,\n",
       " 'screen version': 751,\n",
       " 'version The': 752,\n",
       " 'The Quiet': 753,\n",
       " 'Quiet American': 754,\n",
       " 'American .': 755,\n",
       " 'The Pianist': 756,\n",
       " 'Pianist lacks': 757,\n",
       " 'lacks quick': 758,\n",
       " 'quick emotional': 759,\n",
       " 'emotional connections': 760,\n",
       " 'connections Steven': 761,\n",
       " 'Steven Spielberg': 762,\n",
       " 'Spielberg Schindler': 763,\n",
       " 'Schindler List': 764,\n",
       " 'List .': 765,\n",
       " 'Inventive ,': 766,\n",
       " 'fun ,': 767,\n",
       " ', intoxicatingly': 768,\n",
       " 'intoxicatingly sexy': 769,\n",
       " 'sexy ,': 770,\n",
       " ', violent': 771,\n",
       " 'violent ,': 772,\n",
       " ', self-indulgent': 773,\n",
       " 'self-indulgent maddening': 774,\n",
       " 'maddening .': 775,\n",
       " 'Tells fascinating': 776,\n",
       " 'fascinating ,': 777,\n",
       " ', compelling': 778,\n",
       " 'compelling story': 779,\n",
       " 'Written ,': 780,\n",
       " ', flatly': 781,\n",
       " 'flatly ,': 782,\n",
       " ', David': 783,\n",
       " 'David Kendall': 784,\n",
       " 'Kendall directed': 785,\n",
       " 'directed ,': 786,\n",
       " ', barely': 787,\n",
       " 'barely ,': 788,\n",
       " ', There': 789,\n",
       " 'There Something': 790,\n",
       " 'Something About': 791,\n",
       " 'About Mary': 792,\n",
       " 'Mary co-writer': 793,\n",
       " 'co-writer Ed': 794,\n",
       " 'Ed Decter': 795,\n",
       " 'Decter .': 796,\n",
       " 'Highly recommended': 797,\n",
       " 'recommended viewing': 798,\n",
       " 'viewing courage': 799,\n",
       " 'courage ,': 800,\n",
       " ', ideas': 801,\n",
       " 'ideas ,': 802,\n",
       " ', technical': 803,\n",
       " 'technical proficiency': 804,\n",
       " 'proficiency acting': 805,\n",
       " 'acting .': 806,\n",
       " 'In IMAX': 807,\n",
       " 'IMAX short': 808,\n",
       " 'short ,': 809,\n",
       " ', wonderful': 810,\n",
       " 'wonderful screen': 811,\n",
       " 'screen .': 812,\n",
       " 'It slice': 813,\n",
       " 'slice life': 814,\n",
       " 'life instantly': 815,\n",
       " 'instantly recognizable': 816,\n",
       " 'recognizable .': 817,\n",
       " 'Amid shock': 818,\n",
       " 'shock curiosity': 819,\n",
       " 'curiosity factors': 820,\n",
       " 'factors ,': 821,\n",
       " ', film': 822,\n",
       " 'film corny': 823,\n",
       " 'corny examination': 824,\n",
       " 'examination actress': 825,\n",
       " 'actress .': 826,\n",
       " 'It lazy': 827,\n",
       " 'lazy movie': 828,\n",
       " 'movie avoid': 829,\n",
       " 'avoid solving': 830,\n",
       " 'solving distract': 831,\n",
       " 'distract solution': 832,\n",
       " 'solution .': 833,\n",
       " '... comprehensible': 834,\n",
       " 'comprehensible Dummies': 835,\n",
       " 'Dummies guide': 836,\n",
       " 'guide ,': 837,\n",
       " ', non-techies': 838,\n",
       " 'non-techies enjoy': 839,\n",
       " 'enjoy .': 840,\n",
       " 'Entertains music': 841,\n",
       " 'music comic': 842,\n",
       " 'comic antics': 843,\n",
       " 'antics ,': 844,\n",
       " ', perverse': 845,\n",
       " 'perverse pleasure': 846,\n",
       " 'pleasure watching': 847,\n",
       " 'watching Disney': 848,\n",
       " 'Disney scrape': 849,\n",
       " 'scrape bottom': 850,\n",
       " 'bottom cracker': 851,\n",
       " 'cracker barrel': 852,\n",
       " 'barrel .': 853,\n",
       " 'Writer\\\\/director Burr': 854,\n",
       " 'Burr Steers': 855,\n",
       " 'Steers emphasizes': 856,\n",
       " 'emphasizes Q': 857,\n",
       " 'Q Quirky': 858,\n",
       " 'Quirky ,': 859,\n",
       " ', mixed': 860,\n",
       " 'mixed .': 861,\n",
       " 'Scarcely worth': 862,\n",
       " 'worth mention': 863,\n",
       " 'mention reporting': 864,\n",
       " 'reporting tumbleweeds': 865,\n",
       " 'tumbleweeds blowing': 866,\n",
       " 'blowing empty': 867,\n",
       " 'empty theatres': 868,\n",
       " 'theatres graced': 869,\n",
       " 'graced company': 870,\n",
       " 'company .': 871,\n",
       " 'Invigorating ,': 872,\n",
       " ', surreal': 873,\n",
       " 'surreal ,': 874,\n",
       " ', resonant': 875,\n",
       " 'resonant rainbow': 876,\n",
       " 'rainbow emotion': 877,\n",
       " 'emotion .': 878,\n",
       " '`` Analyze': 879,\n",
       " 'Analyze That': 880,\n",
       " \"That ''\": 881,\n",
       " \"'' crass\": 882,\n",
       " 'crass ,': 883,\n",
       " ', contrived': 884,\n",
       " 'contrived sequels': 885,\n",
       " 'sequels fails': 886,\n",
       " 'fails ,': 887,\n",
       " ', second-guess': 888,\n",
       " 'second-guess affection': 889,\n",
       " 'affection original': 890,\n",
       " 'original .': 891,\n",
       " 'Report card': 892,\n",
       " 'card :': 893,\n",
       " ': Does': 894,\n",
       " 'Does live': 895,\n",
       " 'live exalted': 896,\n",
       " 'exalted tagline': 897,\n",
       " 'tagline -': 898,\n",
       " '- definite': 899,\n",
       " 'definite improvement': 900,\n",
       " 'improvement .': 901,\n",
       " 'It film': 902,\n",
       " ', monsterous': 903,\n",
       " 'monsterous .': 904,\n",
       " 'Tsai ploughing': 905,\n",
       " 'ploughing furrow': 906,\n",
       " 'furrow .': 907,\n",
       " '-LRB- Green': 908,\n",
       " 'Green -RRB-': 909,\n",
       " '-RRB- comedy': 910,\n",
       " 'comedy equivalent': 911,\n",
       " 'equivalent Saddam': 912,\n",
       " 'Saddam Hussein': 913,\n",
       " 'Hussein ,': 914,\n",
       " 'I ready': 915,\n",
       " 'ready U.N.': 916,\n",
       " 'U.N. permission': 917,\n",
       " 'permission preemptive': 918,\n",
       " 'preemptive strike': 919,\n",
       " 'strike .': 920,\n",
       " 'A surprisingly': 921,\n",
       " 'surprisingly `': 922,\n",
       " '` solid': 923,\n",
       " \"solid '\": 924,\n",
       " \"' achievement\": 925,\n",
       " 'achievement director': 926,\n",
       " 'director Malcolm': 927,\n",
       " 'Malcolm D.': 928,\n",
       " 'D. Lee': 929,\n",
       " 'Lee writer': 930,\n",
       " 'writer John': 931,\n",
       " 'John Ridley': 932,\n",
       " 'Ridley .': 933,\n",
       " 'Massoud story': 934,\n",
       " 'story epic': 935,\n",
       " 'epic ,': 936,\n",
       " ', tragedy': 937,\n",
       " 'tragedy ,': 938,\n",
       " ', record': 939,\n",
       " 'record tenacious': 940,\n",
       " 'tenacious ,': 941,\n",
       " ', humane': 942,\n",
       " 'humane fighter': 943,\n",
       " 'fighter prisoner': 944,\n",
       " 'prisoner -LRB-': 945,\n",
       " '-LRB- ultimately': 946,\n",
       " 'ultimately victim': 947,\n",
       " 'victim -RRB-': 948,\n",
       " '-RRB- history': 949,\n",
       " 'history .': 950,\n",
       " 'Changing Lanes': 951,\n",
       " 'Lanes .': 952,\n",
       " 'Shyamalan mom': 953,\n",
       " 'mom .': 954,\n",
       " 'A macabre': 955,\n",
       " 'macabre stylized': 956,\n",
       " 'stylized Swedish': 957,\n",
       " 'Swedish fillm': 958,\n",
       " 'fillm modern': 959,\n",
       " 'modern city': 960,\n",
       " 'city religious': 961,\n",
       " 'religious civic': 962,\n",
       " 'civic virtues': 963,\n",
       " 'virtues hold': 964,\n",
       " 'hold society': 965,\n",
       " 'society tatters': 966,\n",
       " 'tatters .': 967,\n",
       " 'One memory': 968,\n",
       " 'memory thoughtful': 969,\n",
       " 'thoughtful films': 970,\n",
       " 'films art': 971,\n",
       " 'art ,': 972,\n",
       " ', ethics': 973,\n",
       " 'ethics ,': 974,\n",
       " ', cost': 975,\n",
       " 'cost moral': 976,\n",
       " 'moral compromise': 977,\n",
       " 'compromise .': 978,\n",
       " 'The narrative': 979,\n",
       " 'narrative consistently': 980,\n",
       " 'consistently unimaginative': 981,\n",
       " 'unimaginative saved': 982,\n",
       " 'saved film': 983,\n",
       " 'film aid': 984,\n",
       " 'aid wisecracking': 985,\n",
       " 'wisecracking Mystery': 986,\n",
       " 'Mystery Science': 987,\n",
       " 'Science Theater': 988,\n",
       " 'Theater 3000': 989,\n",
       " '3000 guys': 990,\n",
       " 'guys .': 991,\n",
       " 'Filmmaker Tian': 992,\n",
       " 'Tian Zhuangzhuang': 993,\n",
       " 'Zhuangzhuang triumphantly': 994,\n",
       " 'triumphantly returns': 995,\n",
       " 'returns narrative': 996,\n",
       " 'narrative filmmaking': 997,\n",
       " 'filmmaking visually': 998,\n",
       " 'visually masterful': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "word_to_ix = words_dic\n",
    "EMBEDDING_DIM = 100 # 词向量的维度\n",
    "embeds = nn.Embedding(dic_length, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
       "         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
       "         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n",
       "          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n",
       "          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n",
       "         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n",
       "         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n",
       "          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479,\n",
       "         -0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n",
       "         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n",
       "         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n",
       "          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n",
       "          0.6614,  1.1899,  0.8165, -0.9135]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "embeds(autograd.Variable(torch.LongTensor([word_to_ix[\"A series\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把y转换成one-hot向量\n",
    "def to_y(y):\n",
    "    y_num = 5\n",
    "    res = np.zeros((y_num, 1))\n",
    "    res[y] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in train.index:\n",
    "    y.append(train['Sentiment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把x转换成one-hot向量\n",
    "def to_X(words, length):\n",
    "    res =torch.zeros([1, 100])\n",
    "    words_length = len(words)\n",
    "#     print(res)\n",
    "    for j in range(words_length-1):\n",
    "        word = words[j] + \" \" + words[j+1]\n",
    "        res += embeds(autograd.Variable(torch.LongTensor([word_to_ix[word]])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in train.index:\n",
    "    words = train['Phrase'][i].split(' ')\n",
    "    words_ = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            words_ =  words_ + \" \" + word\n",
    "    words = words_.strip().split()\n",
    "    X.append(to_X(words,EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram(\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "class n_gram(nn.Module):\n",
    "    def __init__(self, n_dim, tag_size):\n",
    "        super(n_gram, self).__init__()\n",
    "        self.fc = nn.Linear(n_dim, tag_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.dropout(x, p=0.2)\n",
    "        output = nn.functional.log_softmax(x, dim=-1)\n",
    "        return output\n",
    "        \n",
    "model = n_gram(EMBEDDING_DIM, y_num)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Train Loss: 1.432325, Acc: 0.455841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yinzh\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.421885, Acc: 0.464650\n",
      "epoch 2\n",
      "Train Loss: 1.431565, Acc: 0.460400\n",
      "Test Loss: 1.420933, Acc: 0.450980\n",
      "epoch 3\n",
      "Train Loss: 1.434614, Acc: 0.457928\n",
      "Test Loss: 1.424881, Acc: 0.447264\n",
      "epoch 4\n",
      "Train Loss: 1.432096, Acc: 0.457928\n",
      "Test Loss: 1.414831, Acc: 0.462173\n",
      "epoch 5\n",
      "Train Loss: 1.433252, Acc: 0.456930\n",
      "Test Loss: 1.419857, Acc: 0.462557\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    # training-----------------------------\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for (batch_x, batch_y) in list(zip(X_train, y_train)):\n",
    "        batch_y_ = torch.zeros([1])\n",
    "        batch_y_[0] = batch_y\n",
    "        batch_y = batch_y_\n",
    "        batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "        \n",
    "        out = model(batch_x)\n",
    "#         print(out.shape, batch_y.shape)\n",
    "        \n",
    "        loss = loss_func(out, batch_y.long())\n",
    "        train_loss += loss.item()\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        train_correct = (pred == batch_y).sum()\n",
    "        train_acc += train_correct.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.cuda()\n",
    "        optimizer.step()\n",
    "    print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "        y_train)), train_acc / (len(y_train))))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for (batch_x, batch_y) in list(zip(X_test, y_test)):\n",
    "        batch_y_ = torch.zeros([1])\n",
    "        batch_y_[0] = batch_y\n",
    "        batch_y = batch_y_\n",
    "        batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "        \n",
    "        out = model(batch_x)\n",
    "        \n",
    "        loss = loss_func(out, batch_y.long())\n",
    "        eval_loss += loss.item()\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        num_correct = (pred == batch_y).sum()\n",
    "        eval_acc += num_correct.item()\n",
    "    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        y_test)), eval_acc / (len(y_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 双向LSTM网络进行情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(train['Phrase'], train['Sentiment'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A strangely stirring experience that finds warmth in the coldest environment and makes each crumb of emotional comfort',\n",
       "  3),\n",
       " ('psychedelic', 2),\n",
       " ('of the actress-producer and writer', 2),\n",
       " ('The Bard as black comedy -- Willie would have loved it', 3),\n",
       " (\"if Argento 's Hollywood counterparts\", 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    \n",
    "    return [tokenizer(review) for review, _ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 14141\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab(train_data)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess(test_data, vocab))\n",
    "\n",
    "# 上面的代码等价于下面的注释代码\n",
    "# train_features, train_labels = preprocess(train_data, vocab)\n",
    "# test_features, test_labels = preprocess(test_data, vocab)\n",
    "# train_set = Data.TensorDataset(train_features, train_labels)\n",
    "# test_set = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# len(train_set) = features.shape[0] or labels.shape[0]\n",
    "# train_set[index] = (features[index], labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n",
      "#batches: 1707\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            num_hiddens: 隐藏状态维度大小\n",
    "            num_layers: 隐藏层个数\n",
    "        '''\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # encoder-decoder framework\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 5) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n",
    "        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n",
    "        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n",
    "        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n",
    "        outs = self.decoder(encoding) # (batch_size, 5)\n",
    "        return outs\n",
    "\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载预训练的词向量\n",
    "\n",
    "由于预训练词向量的词典及词语索引与我们使用的数据集并不相同，所以需要根据目前的词典及索引的顺序来加载预训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/400000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1118/400000 [00:00<00:35, 11144.04it/s]\u001b[A\n",
      "  1%|          | 2130/400000 [00:00<00:36, 10801.49it/s]\u001b[A\n",
      "  1%|          | 3769/400000 [00:00<00:32, 12022.64it/s]\u001b[A\n",
      "  1%|▏         | 5231/400000 [00:00<00:31, 12683.79it/s]\u001b[A\n",
      "  2%|▏         | 6867/400000 [00:00<00:28, 13590.65it/s]\u001b[A\n",
      "  2%|▏         | 8232/400000 [00:00<00:28, 13608.27it/s]\u001b[A\n",
      "  2%|▏         | 9894/400000 [00:00<00:27, 14358.64it/s]\u001b[A\n",
      "  3%|▎         | 11367/400000 [00:00<00:26, 14466.67it/s]\u001b[A\n",
      "  3%|▎         | 12896/400000 [00:00<00:26, 14699.77it/s]\u001b[A\n",
      "  4%|▎         | 14339/400000 [00:01<00:26, 14492.36it/s]\u001b[A\n",
      "  4%|▍         | 15896/400000 [00:01<00:25, 14797.83it/s]\u001b[A\n",
      "  4%|▍         | 17590/400000 [00:01<00:24, 15346.57it/s]\u001b[A\n",
      "  5%|▍         | 19284/400000 [00:01<00:24, 15782.16it/s]\u001b[A\n",
      "  5%|▌         | 21070/400000 [00:01<00:23, 16314.06it/s]\u001b[A\n",
      "  6%|▌         | 22895/400000 [00:01<00:22, 16833.65it/s]\u001b[A\n",
      "  6%|▌         | 24741/400000 [00:01<00:21, 17256.84it/s]\u001b[A\n",
      "  7%|▋         | 26579/400000 [00:01<00:21, 17542.34it/s]\u001b[A\n",
      "  7%|▋         | 28341/400000 [00:01<00:22, 16849.96it/s]\u001b[A\n",
      "  8%|▊         | 30037/400000 [00:01<00:23, 16068.56it/s]\u001b[A\n",
      "  8%|▊         | 31708/400000 [00:02<00:22, 16230.22it/s]\u001b[A\n",
      "  8%|▊         | 33342/400000 [00:02<00:23, 15907.45it/s]\u001b[A\n",
      "  9%|▉         | 35024/400000 [00:02<00:22, 16128.37it/s]\u001b[A\n",
      "  9%|▉         | 36829/400000 [00:02<00:21, 16633.61it/s]\u001b[A\n",
      " 10%|▉         | 38600/400000 [00:02<00:21, 16923.53it/s]\u001b[A\n",
      " 10%|█         | 40346/400000 [00:02<00:21, 17032.95it/s]\u001b[A\n",
      " 11%|█         | 42168/400000 [00:02<00:20, 17358.54it/s]\u001b[A\n",
      " 11%|█         | 43952/400000 [00:02<00:20, 17454.83it/s]\u001b[A\n",
      " 11%|█▏        | 45702/400000 [00:02<00:20, 17155.03it/s]\u001b[A\n",
      " 12%|█▏        | 47422/400000 [00:02<00:21, 16379.59it/s]\u001b[A\n",
      " 12%|█▏        | 49087/400000 [00:03<00:21, 16413.87it/s]\u001b[A\n",
      " 13%|█▎        | 50859/400000 [00:03<00:20, 16783.78it/s]\u001b[A\n",
      " 13%|█▎        | 52619/400000 [00:03<00:20, 16982.07it/s]\u001b[A\n",
      " 14%|█▎        | 54392/400000 [00:03<00:20, 17194.04it/s]\u001b[A\n",
      " 14%|█▍        | 56133/400000 [00:03<00:19, 17232.63it/s]\u001b[A\n",
      " 14%|█▍        | 57860/400000 [00:03<00:20, 17048.70it/s]\u001b[A\n",
      " 15%|█▍        | 59568/400000 [00:03<00:20, 16948.25it/s]\u001b[A\n",
      " 15%|█▌        | 61265/400000 [00:03<00:20, 16866.58it/s]\u001b[A\n",
      " 16%|█▌        | 62954/400000 [00:03<00:21, 15651.49it/s]\u001b[A\n",
      " 16%|█▌        | 64538/400000 [00:04<00:21, 15403.43it/s]\u001b[A\n",
      " 17%|█▋        | 66093/400000 [00:04<00:22, 14953.61it/s]\u001b[A\n",
      " 17%|█▋        | 67707/400000 [00:04<00:21, 15289.36it/s]\u001b[A\n",
      " 17%|█▋        | 69572/400000 [00:04<00:20, 16153.70it/s]\u001b[A\n",
      " 18%|█▊        | 71412/400000 [00:04<00:19, 16730.02it/s]\u001b[A\n",
      " 18%|█▊        | 73105/400000 [00:04<00:19, 16654.73it/s]\u001b[A\n",
      " 19%|█▊        | 74835/400000 [00:04<00:19, 16829.00it/s]\u001b[A\n",
      " 19%|█▉        | 76550/400000 [00:04<00:19, 16875.82it/s]\u001b[A\n",
      " 20%|█▉        | 78248/400000 [00:04<00:19, 16894.15it/s]\u001b[A\n",
      " 20%|█▉        | 79943/400000 [00:04<00:19, 16575.16it/s]\u001b[A\n",
      " 20%|██        | 81606/400000 [00:05<00:19, 16351.07it/s]\u001b[A\n",
      " 21%|██        | 83272/400000 [00:05<00:19, 16394.90it/s]\u001b[A\n",
      " 21%|██▏       | 85039/400000 [00:05<00:18, 16718.38it/s]\u001b[A\n",
      " 22%|██▏       | 86781/400000 [00:05<00:18, 16918.96it/s]\u001b[A\n",
      " 22%|██▏       | 88613/400000 [00:05<00:18, 17280.18it/s]\u001b[A\n",
      " 23%|██▎       | 90356/400000 [00:05<00:17, 17280.66it/s]\u001b[A\n",
      " 23%|██▎       | 92097/400000 [00:05<00:17, 17287.45it/s]\u001b[A\n",
      " 23%|██▎       | 93828/400000 [00:05<00:18, 16769.52it/s]\u001b[A\n",
      " 24%|██▍       | 95510/400000 [00:05<00:18, 16339.64it/s]\u001b[A\n",
      " 24%|██▍       | 97150/400000 [00:05<00:18, 16164.98it/s]\u001b[A\n",
      " 25%|██▍       | 98917/400000 [00:06<00:18, 16572.37it/s]\u001b[A\n",
      " 25%|██▌       | 100602/400000 [00:06<00:17, 16651.95it/s]\u001b[A\n",
      " 26%|██▌       | 102317/400000 [00:06<00:17, 16790.93it/s]\u001b[A\n",
      " 26%|██▌       | 104113/400000 [00:06<00:17, 17079.98it/s]\u001b[A\n",
      " 26%|██▋       | 105856/400000 [00:06<00:17, 17141.40it/s]\u001b[A\n",
      " 27%|██▋       | 107612/400000 [00:06<00:16, 17253.24it/s]\u001b[A\n",
      " 27%|██▋       | 109389/400000 [00:06<00:16, 17360.68it/s]\u001b[A\n",
      " 28%|██▊       | 111210/400000 [00:06<00:16, 17592.11it/s]\u001b[A\n",
      " 28%|██▊       | 112971/400000 [00:06<00:16, 17488.25it/s]\u001b[A\n",
      " 29%|██▊       | 114722/400000 [00:06<00:16, 17293.97it/s]\u001b[A\n",
      " 29%|██▉       | 116453/400000 [00:07<00:16, 17281.18it/s]\u001b[A\n",
      " 30%|██▉       | 118183/400000 [00:07<00:16, 17212.09it/s]\u001b[A\n",
      " 30%|██▉       | 119905/400000 [00:07<00:16, 16821.26it/s]\u001b[A\n",
      " 30%|███       | 121657/400000 [00:07<00:16, 17012.75it/s]\u001b[A\n",
      " 31%|███       | 123371/400000 [00:07<00:16, 17017.47it/s]\u001b[A\n",
      " 31%|███▏      | 125089/400000 [00:07<00:16, 17043.06it/s]\u001b[A\n",
      " 32%|███▏      | 126795/400000 [00:07<00:16, 17029.69it/s]\u001b[A\n",
      " 32%|███▏      | 128508/400000 [00:07<00:15, 17046.05it/s]\u001b[A\n",
      " 33%|███▎      | 130214/400000 [00:07<00:16, 16206.75it/s]\u001b[A\n",
      " 33%|███▎      | 131844/400000 [00:08<00:16, 15902.31it/s]\u001b[A\n",
      " 33%|███▎      | 133587/400000 [00:08<00:16, 16295.37it/s]\u001b[A\n",
      " 34%|███▍      | 135324/400000 [00:08<00:15, 16556.31it/s]\u001b[A\n",
      " 34%|███▍      | 136994/400000 [00:08<00:15, 16598.05it/s]\u001b[A\n",
      " 35%|███▍      | 138659/400000 [00:08<00:15, 16606.88it/s]\u001b[A\n",
      " 35%|███▌      | 140422/400000 [00:08<00:15, 16867.01it/s]\u001b[A\n",
      " 36%|███▌      | 142154/400000 [00:08<00:15, 16957.20it/s]\u001b[A\n",
      " 36%|███▌      | 143873/400000 [00:08<00:15, 17013.74it/s]\u001b[A\n",
      " 36%|███▋      | 145642/400000 [00:08<00:14, 17173.76it/s]\u001b[A\n",
      " 37%|███▋      | 147361/400000 [00:08<00:14, 17135.82it/s]\u001b[A\n",
      " 37%|███▋      | 149076/400000 [00:09<00:15, 16659.65it/s]\u001b[A\n",
      " 38%|███▊      | 150746/400000 [00:09<00:15, 16330.91it/s]\u001b[A\n",
      " 38%|███▊      | 152504/400000 [00:09<00:14, 16654.86it/s]\u001b[A\n",
      " 39%|███▊      | 154376/400000 [00:09<00:14, 17222.87it/s]\u001b[A\n",
      " 39%|███▉      | 156185/400000 [00:09<00:13, 17450.32it/s]\u001b[A\n",
      " 40%|███▉      | 158012/400000 [00:09<00:13, 17666.27it/s]\u001b[A\n",
      " 40%|███▉      | 159784/400000 [00:09<00:13, 17662.48it/s]\u001b[A\n",
      " 40%|████      | 161595/400000 [00:09<00:13, 17747.42it/s]\u001b[A\n",
      " 41%|████      | 163442/400000 [00:09<00:13, 17914.20it/s]\u001b[A\n",
      " 41%|████▏     | 165236/400000 [00:09<00:13, 17482.88it/s]\u001b[A\n",
      " 42%|████▏     | 166989/400000 [00:10<00:13, 17191.65it/s]\u001b[A\n",
      " 42%|████▏     | 168712/400000 [00:10<00:13, 16908.28it/s]\u001b[A\n",
      " 43%|████▎     | 170407/400000 [00:10<00:13, 16783.74it/s]\u001b[A\n",
      " 43%|████▎     | 172091/400000 [00:10<00:13, 16800.48it/s]\u001b[A\n",
      " 43%|████▎     | 173826/400000 [00:10<00:13, 16925.59it/s]\u001b[A\n",
      " 44%|████▍     | 175521/400000 [00:10<00:13, 16679.08it/s]\u001b[A\n",
      " 44%|████▍     | 177191/400000 [00:10<00:13, 16540.39it/s]\u001b[A\n",
      " 45%|████▍     | 178950/400000 [00:10<00:13, 16828.84it/s]\u001b[A\n",
      " 45%|████▌     | 180839/400000 [00:10<00:12, 17387.86it/s]\u001b[A\n",
      " 46%|████▌     | 182588/400000 [00:10<00:12, 17398.29it/s]\u001b[A\n",
      " 46%|████▌     | 184333/400000 [00:11<00:12, 17160.60it/s]\u001b[A\n",
      " 47%|████▋     | 186053/400000 [00:11<00:12, 17134.59it/s]\u001b[A\n",
      " 47%|████▋     | 187770/400000 [00:11<00:12, 17046.55it/s]\u001b[A\n",
      " 47%|████▋     | 189477/400000 [00:11<00:12, 16517.57it/s]\u001b[A\n",
      " 48%|████▊     | 191134/400000 [00:11<00:12, 16501.70it/s]\u001b[A\n",
      " 48%|████▊     | 192788/400000 [00:11<00:12, 16197.33it/s]\u001b[A\n",
      " 49%|████▊     | 194412/400000 [00:11<00:13, 15811.16it/s]\u001b[A\n",
      " 49%|████▉     | 196079/400000 [00:11<00:12, 16015.97it/s]\u001b[A\n",
      " 49%|████▉     | 197685/400000 [00:11<00:12, 15924.81it/s]\u001b[A\n",
      " 50%|████▉     | 199281/400000 [00:12<00:13, 14642.40it/s]\u001b[A\n",
      " 50%|█████     | 200910/400000 [00:12<00:13, 15096.73it/s]\u001b[A\n",
      " 51%|█████     | 202439/400000 [00:12<00:13, 14927.32it/s]\u001b[A\n",
      " 51%|█████     | 204054/400000 [00:12<00:12, 15233.61it/s]\u001b[A\n",
      " 51%|█████▏    | 205681/400000 [00:12<00:12, 15503.06it/s]\u001b[A\n",
      " 52%|█████▏    | 207405/400000 [00:12<00:12, 15983.26it/s]\u001b[A\n",
      " 52%|█████▏    | 209186/400000 [00:12<00:11, 16464.66it/s]\u001b[A\n",
      " 53%|█████▎    | 210920/400000 [00:12<00:11, 16684.81it/s]\u001b[A\n",
      " 53%|█████▎    | 212688/400000 [00:12<00:11, 16971.36it/s]\u001b[A\n",
      " 54%|█████▎    | 214426/400000 [00:12<00:10, 17090.39it/s]\u001b[A\n",
      " 54%|█████▍    | 216141/400000 [00:13<00:10, 17052.09it/s]\u001b[A\n",
      " 54%|█████▍    | 217850/400000 [00:13<00:10, 16840.10it/s]\u001b[A\n",
      " 55%|█████▍    | 219630/400000 [00:13<00:10, 17095.18it/s]\u001b[A\n",
      " 55%|█████▌    | 221478/400000 [00:13<00:10, 17450.06it/s]\u001b[A\n",
      " 56%|█████▌    | 223227/400000 [00:13<00:10, 17300.37it/s]\u001b[A\n",
      " 56%|█████▌    | 224961/400000 [00:13<00:10, 17037.00it/s]\u001b[A\n",
      " 57%|█████▋    | 226743/400000 [00:13<00:10, 17243.46it/s]\u001b[A\n",
      " 57%|█████▋    | 228578/400000 [00:13<00:09, 17549.65it/s]\u001b[A\n",
      " 58%|█████▊    | 230375/400000 [00:13<00:09, 17659.75it/s]\u001b[A\n",
      " 58%|█████▊    | 232144/400000 [00:13<00:09, 17398.03it/s]\u001b[A\n",
      " 58%|█████▊    | 233887/400000 [00:14<00:10, 15889.83it/s]\u001b[A\n",
      " 59%|█████▉    | 235504/400000 [00:14<00:10, 15442.64it/s]\u001b[A\n",
      " 59%|█████▉    | 237121/400000 [00:14<00:10, 15644.33it/s]\u001b[A\n",
      " 60%|█████▉    | 238702/400000 [00:14<00:10, 15596.06it/s]\u001b[A\n",
      " 60%|██████    | 240371/400000 [00:14<00:10, 15882.24it/s]\u001b[A\n",
      " 60%|██████    | 242000/400000 [00:14<00:09, 15996.59it/s]\u001b[A\n",
      " 61%|██████    | 243625/400000 [00:14<00:09, 16061.48it/s]\u001b[A\n",
      " 61%|██████▏   | 245442/400000 [00:14<00:09, 16595.72it/s]\u001b[A\n",
      " 62%|██████▏   | 247110/400000 [00:14<00:09, 16290.18it/s]\u001b[A\n",
      " 62%|██████▏   | 248746/400000 [00:15<00:09, 15595.76it/s]\u001b[A\n",
      " 63%|██████▎   | 250317/400000 [00:15<00:09, 15060.90it/s]\u001b[A\n",
      " 63%|██████▎   | 251835/400000 [00:15<00:10, 14660.39it/s]\u001b[A\n",
      " 63%|██████▎   | 253509/400000 [00:15<00:09, 15206.65it/s]\u001b[A\n",
      " 64%|██████▍   | 255083/400000 [00:15<00:09, 15340.39it/s]\u001b[A\n",
      " 64%|██████▍   | 256782/400000 [00:15<00:09, 15769.75it/s]\u001b[A\n",
      " 65%|██████▍   | 258466/400000 [00:15<00:08, 16061.07it/s]\u001b[A\n",
      " 65%|██████▌   | 260080/400000 [00:15<00:08, 15947.02it/s]\u001b[A\n",
      " 65%|██████▌   | 261720/400000 [00:15<00:08, 16043.01it/s]\u001b[A\n",
      " 66%|██████▌   | 263396/400000 [00:15<00:08, 16250.64it/s]\u001b[A\n",
      " 66%|██████▋   | 265025/400000 [00:16<00:08, 16251.86it/s]\u001b[A\n",
      " 67%|██████▋   | 266732/400000 [00:16<00:08, 16475.48it/s]\u001b[A\n",
      " 67%|██████▋   | 268383/400000 [00:16<00:08, 16042.94it/s]\u001b[A\n",
      " 68%|██████▊   | 270107/400000 [00:16<00:07, 16349.03it/s]\u001b[A\n",
      " 68%|██████▊   | 271893/400000 [00:16<00:07, 16773.16it/s]\u001b[A\n",
      " 68%|██████▊   | 273653/400000 [00:16<00:07, 16977.59it/s]\u001b[A\n",
      " 69%|██████▉   | 275356/400000 [00:16<00:07, 16724.53it/s]\u001b[A\n",
      " 69%|██████▉   | 277033/400000 [00:16<00:07, 16696.79it/s]\u001b[A\n",
      " 70%|██████▉   | 278706/400000 [00:16<00:07, 16590.19it/s]\u001b[A\n",
      " 70%|███████   | 280368/400000 [00:17<00:07, 15936.90it/s]\u001b[A\n",
      " 70%|███████   | 281991/400000 [00:17<00:07, 16011.91it/s]\u001b[A\n",
      " 71%|███████   | 283598/400000 [00:17<00:07, 15437.97it/s]\u001b[A\n",
      " 71%|███████▏  | 285150/400000 [00:17<00:07, 15206.95it/s]\u001b[A\n",
      " 72%|███████▏  | 286940/400000 [00:17<00:07, 15907.26it/s]\u001b[A\n",
      " 72%|███████▏  | 288753/400000 [00:17<00:06, 16490.36it/s]\u001b[A\n",
      " 73%|███████▎  | 290579/400000 [00:17<00:06, 16960.44it/s]\u001b[A\n",
      " 73%|███████▎  | 292376/400000 [00:17<00:06, 17218.18it/s]\u001b[A\n",
      " 74%|███████▎  | 294109/400000 [00:17<00:06, 16925.93it/s]\u001b[A\n",
      " 74%|███████▍  | 295859/400000 [00:17<00:06, 17057.55it/s]\u001b[A\n",
      " 74%|███████▍  | 297571/400000 [00:18<00:06, 16082.60it/s]\u001b[A\n",
      " 75%|███████▍  | 299196/400000 [00:18<00:06, 14799.49it/s]\u001b[A\n",
      " 75%|███████▌  | 300708/400000 [00:18<00:06, 14760.12it/s]\u001b[A\n",
      " 76%|███████▌  | 302417/400000 [00:18<00:06, 15366.47it/s]\u001b[A\n",
      " 76%|███████▌  | 304161/400000 [00:18<00:06, 15895.64it/s]\u001b[A\n",
      " 76%|███████▋  | 305934/400000 [00:18<00:05, 16361.95it/s]\u001b[A\n",
      " 77%|███████▋  | 307694/400000 [00:18<00:05, 16705.47it/s]\u001b[A\n",
      " 77%|███████▋  | 309506/400000 [00:18<00:05, 17064.74it/s]\u001b[A\n",
      " 78%|███████▊  | 311246/400000 [00:18<00:05, 17115.65it/s]\u001b[A\n",
      " 78%|███████▊  | 312966/400000 [00:18<00:05, 17025.75it/s]\u001b[A\n",
      " 79%|███████▊  | 314675/400000 [00:19<00:05, 16440.62it/s]\u001b[A\n",
      " 79%|███████▉  | 316328/400000 [00:19<00:05, 15907.98it/s]\u001b[A\n",
      " 79%|███████▉  | 317929/400000 [00:19<00:05, 15772.56it/s]\u001b[A\n",
      " 80%|███████▉  | 319597/400000 [00:19<00:05, 16004.64it/s]\u001b[A\n",
      " 80%|████████  | 321249/400000 [00:19<00:04, 16115.61it/s]\u001b[A\n",
      " 81%|████████  | 322865/400000 [00:19<00:04, 15932.68it/s]\u001b[A\n",
      " 81%|████████  | 324462/400000 [00:19<00:04, 15899.99it/s]\u001b[A\n",
      " 82%|████████▏ | 326080/400000 [00:19<00:04, 15969.46it/s]\u001b[A\n",
      " 82%|████████▏ | 327759/400000 [00:19<00:04, 16206.93it/s]\u001b[A\n",
      " 82%|████████▏ | 329410/400000 [00:20<00:04, 16258.53it/s]\u001b[A\n",
      " 83%|████████▎ | 331086/400000 [00:20<00:04, 16371.97it/s]\u001b[A\n",
      " 83%|████████▎ | 332725/400000 [00:20<00:04, 16221.46it/s]\u001b[A\n",
      " 84%|████████▎ | 334398/400000 [00:20<00:04, 16362.04it/s]\u001b[A\n",
      " 84%|████████▍ | 336036/400000 [00:20<00:03, 16090.79it/s]\u001b[A\n",
      " 84%|████████▍ | 337647/400000 [00:20<00:04, 15521.31it/s]\u001b[A\n",
      " 85%|████████▍ | 339244/400000 [00:20<00:03, 15618.63it/s]\u001b[A\n",
      " 85%|████████▌ | 340914/400000 [00:20<00:03, 15891.72it/s]\u001b[A\n",
      " 86%|████████▌ | 342508/400000 [00:20<00:03, 15855.19it/s]\u001b[A\n",
      " 86%|████████▌ | 344097/400000 [00:20<00:03, 14841.20it/s]\u001b[A\n",
      " 86%|████████▋ | 345703/400000 [00:21<00:03, 15186.57it/s]\u001b[A\n",
      " 87%|████████▋ | 347369/400000 [00:21<00:03, 15590.89it/s]\u001b[A\n",
      " 87%|████████▋ | 349086/400000 [00:21<00:03, 16008.56it/s]\u001b[A\n",
      " 88%|████████▊ | 350702/400000 [00:21<00:03, 16022.16it/s]\u001b[A\n",
      " 88%|████████▊ | 352312/400000 [00:21<00:03, 15870.34it/s]\u001b[A\n",
      " 89%|████████▊ | 354045/400000 [00:21<00:02, 16238.04it/s]\u001b[A\n",
      " 89%|████████▉ | 355732/400000 [00:21<00:02, 16408.61it/s]\u001b[A\n",
      " 89%|████████▉ | 357400/400000 [00:21<00:02, 16453.16it/s]\u001b[A\n",
      " 90%|████████▉ | 359049/400000 [00:21<00:02, 16428.01it/s]\u001b[A\n",
      " 90%|█████████ | 360790/400000 [00:21<00:02, 16702.40it/s]\u001b[A\n",
      " 91%|█████████ | 362492/400000 [00:22<00:02, 16744.93it/s]\u001b[A\n",
      " 91%|█████████ | 364169/400000 [00:22<00:02, 16216.68it/s]\u001b[A\n",
      " 91%|█████████▏| 365796/400000 [00:22<00:02, 15639.58it/s]\u001b[A\n",
      " 92%|█████████▏| 367455/400000 [00:22<00:02, 15897.11it/s]\u001b[A\n",
      " 92%|█████████▏| 369081/400000 [00:22<00:01, 15962.39it/s]\u001b[A\n",
      " 93%|█████████▎| 370753/400000 [00:22<00:01, 16168.13it/s]\u001b[A\n",
      " 93%|█████████▎| 372383/400000 [00:22<00:01, 16164.96it/s]\u001b[A\n",
      " 94%|█████████▎| 374050/400000 [00:22<00:01, 16306.43it/s]\u001b[A\n",
      " 94%|█████████▍| 375725/400000 [00:22<00:01, 16394.19it/s]\u001b[A\n",
      " 94%|█████████▍| 377367/400000 [00:23<00:01, 16135.16it/s]\u001b[A\n",
      " 95%|█████████▍| 378983/400000 [00:23<00:01, 15700.02it/s]\u001b[A\n",
      " 95%|█████████▌| 380558/400000 [00:23<00:01, 15543.33it/s]\u001b[A\n",
      " 96%|█████████▌| 382116/400000 [00:23<00:01, 15481.17it/s]\u001b[A\n",
      " 96%|█████████▌| 383781/400000 [00:23<00:01, 15778.37it/s]\u001b[A\n",
      " 96%|█████████▋| 385362/400000 [00:23<00:00, 15741.91it/s]\u001b[A\n",
      " 97%|█████████▋| 386939/400000 [00:23<00:00, 15616.25it/s]\u001b[A\n",
      " 97%|█████████▋| 388568/400000 [00:23<00:00, 15769.18it/s]\u001b[A\n",
      " 98%|█████████▊| 390204/400000 [00:23<00:00, 15924.73it/s]\u001b[A\n",
      " 98%|█████████▊| 391882/400000 [00:23<00:00, 16137.90it/s]\u001b[A\n",
      " 98%|█████████▊| 393552/400000 [00:24<00:00, 16302.30it/s]\u001b[A\n",
      " 99%|█████████▉| 395184/400000 [00:24<00:00, 16206.70it/s]\u001b[A\n",
      " 99%|█████████▉| 396806/400000 [00:24<00:00, 16121.30it/s]\u001b[A\n",
      "100%|█████████▉| 398535/400000 [00:24<00:00, 16418.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 916 oov words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████▉| 398535/400000 [00:38<00:00, 16418.01it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./\"\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.9642, train acc 0.602, test acc 0.625, time 255.5 sec\n",
      "epoch 2, loss 0.4367, train acc 0.639, test acc 0.638, time 254.3 sec\n",
      "epoch 3, loss 0.2765, train acc 0.655, test acc 0.647, time 255.1 sec\n",
      "epoch 4, loss 0.2010, train acc 0.666, test acc 0.642, time 257.2 sec\n",
      "epoch 5, loss 0.1595, train acc 0.670, test acc 0.647, time 259.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    '''\n",
    "    @params：\n",
    "        net: 训练好的模型\n",
    "        vocab: 在该数据集上创建的词典，用于将给定的单词序转换为单词下标的序列，从而输入模型\n",
    "        sentence: 需要分析情感的文本，以单词序列的形式给出\n",
    "    @return: 预测的结果\n",
    "    0 - negative\n",
    "    1 - somewhat negative\n",
    "    2 - neutral\n",
    "    3 - somewhat positive\n",
    "    4 - positive\n",
    "    '''\n",
    "    device = list(net.parameters())[0].device # 读取模型所在的环境\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    sentiments = {0: \"negative\", 1: \"somewhat negative\", 2: \"neutral\", 3: \"somewhat positive\", 4: \"positive\"}\n",
    "    return sentiments[label.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somewhat positive'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 卷积神经网络进行情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextCNN 模型**\n",
    "\n",
    "TextCNN 模型主要使用了一维卷积层和时序最大池化层。假设输入的文本序列由 n 个词组成，每个词用 d 维的词向量表示。那么输入样本的宽为 n，输入通道数为 d。TextCNN 的计算主要分为以下几步。\n",
    "\n",
    "1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。\n",
    "2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。\n",
    "3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来实现 TextCNN 模型。与上一节相比，除了用一维卷积层替换循环神经网络外，这里我们还使用了两个嵌入层，一个的权重固定，另一个则参与训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.max_pool1d(x,kernel_size=x.shape[2])\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            kernel_sizes: 卷积核大小列表\n",
    "            num_channels: 卷积通道数列表\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # 参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # 不参与训练的嵌入层\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "            \n",
    "        self.decoder = nn.Linear(sum(num_channels), 5)\n",
    "        self.dropout = nn.Dropout(0.5) # 丢弃层用于防止过拟合\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outputs: 对文本情感的预测，形状为 (batch_size, 5) 的张量\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch_size, seq_len, 2*embed_size)\n",
    "        # 根据一维卷积层要求的输入格式，需要将张量进行转置\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # encoding = []\n",
    "        # for conv in self.convs:\n",
    "        #     out = conv(embeddings) # (batch_size, out_channels, seq_len-kernel_size+1)\n",
    "        #     out = self.pool(F.relu(out)) # (batch_size, out_channels, 1)\n",
    "        #     encoding.append(out.squeeze(-1)) # (batch_size, out_channels)\n",
    "        # encoding = torch.cat(encoding) # (batch_size, out_channels_sum)\n",
    "        \n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.1583, train acc 0.544, test acc 0.604, time 145.4 sec\n",
      "epoch 2, loss 0.4847, train acc 0.614, test acc 0.629, time 144.0 sec\n",
      "epoch 3, loss 0.2920, train acc 0.650, test acc 0.649, time 143.8 sec\n",
      "epoch 4, loss 0.2059, train acc 0.673, test acc 0.659, time 143.6 sec\n",
      "epoch 5, loss 0.1566, train acc 0.687, test acc 0.662, time 143.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somewhat negative'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
